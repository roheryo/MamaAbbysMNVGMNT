{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a22f0a4e",
   "metadata": {},
   "source": [
    "# Sales Forecast — Colab conversion of predict_ml_sales.py\n",
    "\n",
    "This Colab notebook converts the `predict_ml_sales.py` analysis into a Colab-friendly workflow that:\n",
    "\n",
    "- Loads sales data (CSV or SQLite),\n",
    "- Reproduces the advanced feature engineering,\n",
    "- Builds a TensorFlow model (sequence model) to forecast next-day sales,\n",
    "- Exports the model as SavedModel and TFLite (float and quantized),\n",
    "- Packages feature order and scaler parameters for embedding into the Flutter app.\n",
    "\n",
    "Usage notes:\n",
    "- Run cells in order. Use the \"Runtime > Change runtime type\" menu in Colab and select GPU if available.\n",
    "- The notebook keeps cells modular so you can re-run preprocessing or model training independently.\n",
    "\n",
    "---\n",
    "\n",
    "Outline: Environment setup → Upload/Mount → Inspect Script → Data loading/EDA → Feature engineering → TF dataset → Model → Train → Export → Convert to TFLite → Validate → Package → Dart integration snippet\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d95d6d9",
   "metadata": {},
   "source": [
    "## 1) Environment setup (Colab + GPU + Packages)\n",
    "\n",
    "This cell installs required packages and verifies the runtime. If you're on Google Colab, enable a GPU runtime: Runtime → Change runtime type → GPU. The install is quiet and idempotent.\n",
    "\n",
    "Run the next Python cell to install packages and check available GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de28e85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "!pip install -q tensorflow pandas numpy matplotlib seaborn scikit-learn tflite-support joblib\n",
    "\n",
    "# Verify GPU\n",
    "!nvidia-smi || true\n",
    "\n",
    "import tensorflow as tf\n",
    "print('TensorFlow version:', tf.__version__)\n",
    "print('GPUs:', tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "import os, random\n",
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "print('Seeds set.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0e6263",
   "metadata": {},
   "source": [
    "## 2) Upload or mount project and inspect `predict_ml_sales.py`\n",
    "\n",
    "Options:\n",
    "- Upload files directly via Colab UI (files.upload),\n",
    "- Mount Google Drive and copy files from your Drive, or\n",
    "- If running locally, place `predict_ml_sales.py` in the notebook folder and inspect.\n",
    "\n",
    "Use the next cell to mount Google Drive or use the file upload widget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f188d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Upload file via Colab UI\n",
    "from google.colab import files\n",
    "print('If you have predict_ml_sales.py locally, upload it now (optional).')\n",
    "# uploaded = files.upload()  # Uncomment to use upload widget\n",
    "\n",
    "# Option B: Mount Google Drive (recommended for larger files)\n",
    "from google.colab import drive\n",
    "print('Mounting Drive to /content/drive ...')\n",
    "drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "# Example path if you placed the repo in Drive\n",
    "# repo_path = '/content/drive/MyDrive/MamaAbbysMNVGMNT'\n",
    "# !ls -la \"{repo_path}\" \n",
    "\n",
    "# If predict_ml_sales.py was uploaded or copied, show first 200 lines\n",
    "import os\n",
    "if os.path.exists('predict_ml_sales.py'):\n",
    "    print('\\nFound predict_ml_sales.py in notebook directory. Showing first 200 lines:')\n",
    "    !sed -n '1,200p' predict_ml_sales.py\n",
    "else:\n",
    "    print('\\npredict_ml_sales.py not found in the notebook directory. If you mounted Drive, copy it to /content or change the path.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4271af",
   "metadata": {},
   "source": [
    "## 3) Inspect script structure & refactor plan\n",
    "\n",
    "We'll reuse the advanced feature engineering from `predict_ml_sales.py` but adapt the training to TensorFlow. The plan:\n",
    "\n",
    "- Load data into `df` (daily sales totals).\n",
    "- Reproduce engineered features found in predict_ml_sales.py (lags, rolling stats, cyclical features).\n",
    "- Create sequences of length `window_size` (e.g., 30) to predict the next day's sales.\n",
    "- Train a compact Keras model and export it for TFLite conversion.\n",
    "\n",
    "Run the next cell to import common libraries used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bfd807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports for the rest of the notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "print('Libraries imported')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680d3419",
   "metadata": {},
   "source": [
    "## 4) Data loading and quick EDA\n",
    "\n",
    "This cell provides three options to load data into `df`:\n",
    "- Option 1: Load CSV you uploaded (uncomment to use),\n",
    "- Option 2: Load CSV from Drive path,\n",
    "- Option 3: Load from a SQLite DB (if you copied the app.db to Colab/Drive) and the `store_sales` table exists.\n",
    "\n",
    "The expected DataFrame should have at least: `sale_date` (date), `sales` (numeric), `holiday_flag` (0/1), `day_of_week`, and `month` (optional). We'll try CSV first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064aac3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: CSV in notebook directory (uncomment when file uploaded)\n",
    "# df = pd.read_csv('sales_daily.csv', parse_dates=['sale_date'])\n",
    "\n",
    "# Option 2: CSV in Drive (change path as needed)\n",
    "drive_csv_path = '/content/drive/MyDrive/MamaAbbysMNVGMNT/sales_daily.csv'\n",
    "if os.path.exists(drive_csv_path):\n",
    "    print('Loading CSV from Drive:', drive_csv_path)\n",
    "    df = pd.read_csv(drive_csv_path, parse_dates=['sale_date'])\n",
    "else:\n",
    "    # Option 3: try reading from copied SQLite database if available\n",
    "    db_path_candidates = [\n",
    "        '/content/drive/MyDrive/MamaAbbysMNVGMNT/app.db',\n",
    "        '/content/app.db',\n",
    "        '/content/drive/MyDrive/app.db'\n",
    "    ]\n",
    "    db_found = None\n",
    "    for p in db_path_candidates:\n",
    "        if os.path.exists(p):\n",
    "            db_found = p\n",
    "            break\n",
    "\n",
    "    if db_found:\n",
    "        print('Loading from SQLite DB:', db_found)\n",
    "        import sqlite3\n",
    "        with sqlite3.connect(db_found) as conn:\n",
    "            query = 'SELECT id, sale_date, day_of_week, month, holiday_flag, sales FROM store_sales ORDER BY sale_date'\n",
    "            df = pd.read_sql_query(query, conn, parse_dates=['sale_date'])\n",
    "    else:\n",
    "        raise FileNotFoundError('No CSV or DB found. Upload sales CSV as sales_daily.csv or place app.db in Drive.')\n",
    "\n",
    "print('Loaded dataframe shape:', df.shape)\n",
    "print(df.head())\n",
    "print('\\nData types:\\n', df.dtypes)\n",
    "\n",
    "# Quick time range\n",
    "print('\\nDate range:', df['sale_date'].min(), '->', df['sale_date'].max())\n",
    "\n",
    "# Basic plot of sales over time\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(df['sale_date'], df['sales'], label='sales')\n",
    "plt.title('Sales over time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14389c3",
   "metadata": {},
   "source": [
    "## 5) Feature engineering (adapted from predict_ml_sales.py)\n",
    "\n",
    "This cell creates date-based features, cyclical encodings, lag features, rolling statistics, EMAs, ratios, and drops NaNs — matching the heavy feature engineering in the original file but kept deterministic for TensorFlow pipelines.\n",
    "\n",
    "You can adjust lag windows and rolling windows as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cca572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure required columns exist\n",
    "if 'sale_date' not in df.columns or 'sales' not in df.columns:\n",
    "    raise ValueError('Dataframe must include sale_date and sales columns')\n",
    "\n",
    "# Copy df to avoid modifying original\n",
    "df_proc = df.copy()\n",
    "\n",
    "# Ensure datetime\n",
    "df_proc['sale_date'] = pd.to_datetime(df_proc['sale_date'])\n",
    "\n",
    "# Fill missing months or holiday flags if absent\n",
    "if 'month' not in df_proc.columns:\n",
    "    df_proc['month'] = df_proc['sale_date'].dt.month\n",
    "if 'holiday_flag' not in df_proc.columns:\n",
    "    df_proc['holiday_flag'] = 0\n",
    "if 'day_of_week' not in df_proc.columns:\n",
    "    df_proc['day_of_week'] = df_proc['sale_date'].dt.day_name()\n",
    "\n",
    "# Sort and set index\n",
    "df_proc = df_proc.sort_values('sale_date').reset_index(drop=True)\n",
    "\n",
    "# Extract date parts\n",
    "df_proc['year'] = df_proc['sale_date'].dt.year\n",
    "df_proc['day'] = df_proc['sale_date'].dt.day\n",
    "df_proc['quarter'] = df_proc['sale_date'].dt.quarter\n",
    "df_proc['weekday'] = df_proc['sale_date'].dt.weekday\n",
    "df_proc['week_of_year'] = df_proc['sale_date'].dt.isocalendar().week\n",
    "df_proc['day_of_year'] = df_proc['sale_date'].dt.dayofyear\n",
    "\n",
    "df_proc['is_weekend'] = (df_proc['weekday'] >= 5).astype(int)\n",
    "df_proc['is_month_start'] = df_proc['sale_date'].dt.is_month_start.astype(int)\n",
    "df_proc['is_month_end'] = df_proc['sale_date'].dt.is_month_end.astype(int)\n",
    "df_proc['is_quarter_start'] = df_proc['sale_date'].dt.is_quarter_start.astype(int)\n",
    "df_proc['is_quarter_end'] = df_proc['sale_date'].dt.is_quarter_end.astype(int)\n",
    "\n",
    "# Map day_of_week to numeric\n",
    "day_mapping = {'Monday':0,'Tuesday':1,'Wednesday':2,'Thursday':3,'Friday':4,'Saturday':5,'Sunday':6}\n",
    "if df_proc['day_of_week'].dtype == object:\n",
    "    df_proc['day_of_week_encoded'] = df_proc['day_of_week'].map(day_mapping).fillna(df_proc['weekday']).astype(int)\n",
    "else:\n",
    "    df_proc['day_of_week_encoded'] = pd.to_numeric(df_proc['day_of_week'], errors='coerce').fillna(df_proc['weekday']).astype(int)\n",
    "\n",
    "# Cyclical features\n",
    "df_proc['month_sin'] = np.sin(2*np.pi*df_proc['month']/12)\n",
    "df_proc['month_cos'] = np.cos(2*np.pi*df_proc['month']/12)\n",
    "df_proc['day_sin'] = np.sin(2*np.pi*df_proc['day']/31)\n",
    "df_proc['day_cos'] = np.cos(2*np.pi*df_proc['day']/31)\n",
    "df_proc['weekday_sin'] = np.sin(2*np.pi*df_proc['weekday']/7)\n",
    "df_proc['weekday_cos'] = np.cos(2*np.pi*df_proc['weekday']/7)\n",
    "df_proc['quarter_sin'] = np.sin(2*np.pi*df_proc['quarter']/4)\n",
    "df_proc['quarter_cos'] = np.cos(2*np.pi*df_proc['quarter']/4)\n",
    "df_proc['week_sin'] = np.sin(2*np.pi*df_proc['week_of_year']/52)\n",
    "df_proc['week_cos'] = np.cos(2*np.pi*df_proc['week_of_year']/52)\n",
    "\n",
    "# Lag features\n",
    "for lag in [1,2,3,7,14,21,30]:\n",
    "    df_proc[f'sales_lag{lag}'] = df_proc['sales'].shift(lag)\n",
    "\n",
    "# Rolling stats\n",
    "for window in [3,7,14,21,30]:\n",
    "    df_proc[f'sales_ma{window}'] = df_proc['sales'].rolling(window=window).mean()\n",
    "    df_proc[f'sales_std{window}'] = df_proc['sales'].rolling(window=window).std()\n",
    "    df_proc[f'sales_min{window}'] = df_proc['sales'].rolling(window=window).min()\n",
    "    df_proc[f'sales_max{window}'] = df_proc['sales'].rolling(window=window).max()\n",
    "    df_proc[f'sales_median{window}'] = df_proc['sales'].rolling(window=window).median()\n",
    "\n",
    "# EMA\n",
    "for span in [7,14,30]:\n",
    "    df_proc[f'sales_ema{span}'] = df_proc['sales'].ewm(span=span).mean()\n",
    "\n",
    "# Trend\n",
    "df_proc['sales_diff1'] = df_proc['sales'].diff(1)\n",
    "df_proc['sales_diff7'] = df_proc['sales'].diff(7)\n",
    "df_proc['sales_pct_change1'] = df_proc['sales'].pct_change(1)\n",
    "df_proc['sales_pct_change7'] = df_proc['sales'].pct_change(7)\n",
    "\n",
    "# Interactions and polynomial\n",
    "df_proc['month_holiday'] = df_proc['month'] * df_proc['holiday_flag']\n",
    "df_proc['weekday_holiday'] = df_proc['weekday'] * df_proc['holiday_flag']\n",
    "df_proc['is_weekend_holiday'] = df_proc['is_weekend'] * df_proc['holiday_flag']\n",
    "df_proc['month_squared'] = df_proc['month']**2\n",
    "df_proc['weekday_squared'] = df_proc['weekday']**2\n",
    "df_proc['day_squared'] = df_proc['day']**2\n",
    "\n",
    "# Ratios and volatility\n",
    "df_proc['sales_to_ma7_ratio'] = df_proc['sales'] / (df_proc['sales_ma7'] + 1e-8)\n",
    "df_proc['sales_to_ma30_ratio'] = df_proc['sales'] / (df_proc['sales_ma30'] + 1e-8)\n",
    "df_proc['ma7_to_ma30_ratio'] = df_proc['sales_ma7'] / (df_proc['sales_ma30'] + 1e-8)\n",
    "\n",
    "df_proc['sales_volatility_7'] = df_proc['sales'].rolling(window=7).std() / (df_proc['sales'].rolling(window=7).mean() + 1e-8)\n",
    "df_proc['sales_volatility_30'] = df_proc['sales'].rolling(window=30).std() / (df_proc['sales'].rolling(window=30).mean() + 1e-8)\n",
    "\n",
    "# Detrended\n",
    "if 'sales_ma30' in df_proc.columns:\n",
    "    df_proc['sales_detrended'] = df_proc['sales'] - df_proc['sales_ma30']\n",
    "\n",
    "# Drop rows with NaNs introduced by lags/rolling\n",
    "df_proc = df_proc.dropna().reset_index(drop=True)\n",
    "\n",
    "print('Processed shape:', df_proc.shape)\n",
    "print('Columns:', df_proc.columns.tolist()[:40])\n",
    "\n",
    "# Keep a ordered feature list (exclude metadata and target)\n",
    "exclude_cols = ['id','sale_date','day_of_week','sales']\n",
    "feature_cols = [c for c in df_proc.columns if c not in exclude_cols]\n",
    "print('Feature count:', len(feature_cols))\n",
    "\n",
    "# Show the first rows\n",
    "df_proc.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e9cd7c",
   "metadata": {},
   "source": [
    "## 6) Create windowed sequences (tf.data) and scale features\n",
    "\n",
    "We'll create sequences of length `window_size` (e.g., 30) using the engineered features to predict the next-day sales. We'll scale features using scikit-learn's StandardScaler and save scaler params for use in Flutter (as JSON/npz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a314803e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# Parameters\n",
    "window_size = 30  # days to use as input\n",
    "forecast_horizon = 1  # predict next day\n",
    "\n",
    "# Prepare X (3D) and y (1D)\n",
    "X_raw = df_proc[feature_cols].values\n",
    "y_raw = df_proc['sales'].values\n",
    "\n",
    "# Create scaler and fit on train portion (time-based split)\n",
    "split_idx = int(len(X_raw) * 0.8)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_raw[:split_idx])\n",
    "X_scaled = scaler.transform(X_raw)\n",
    "\n",
    "# Save scaler params\n",
    "os.makedirs('export', exist_ok=True)\n",
    "joblib.dump(scaler, 'export/scaler.joblib')\n",
    "np.savez('export/scaler_params.npz', mean=scaler.mean_, scale=scaler.scale_)\n",
    "\n",
    "# Build sequences\n",
    "def build_sequences(X, y, window_size):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(window_size, len(X)):\n",
    "        Xs.append(X[i-window_size:i])\n",
    "        ys.append(y[i])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "X_seq, y_seq = build_sequences(X_scaled, y_raw, window_size)\n",
    "print('X_seq shape:', X_seq.shape)\n",
    "print('y_seq shape:', y_seq.shape)\n",
    "\n",
    "# Train/val/test split (time-ordered)\n",
    "train_size = int(0.8 * len(X_seq))\n",
    "X_train, y_train = X_seq[:train_size], y_seq[:train_size]\n",
    "X_val, y_val = X_seq[train_size:], y_seq[train_size:]\n",
    "\n",
    "print('Train shape:', X_train.shape, y_train.shape)\n",
    "print('Val shape:', X_val.shape, y_val.shape)\n",
    "\n",
    "# Create tf.data datasets\n",
    "import tensorflow as tf\n",
    "batch_size = 32\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(1024, seed=seed).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print('Datasets ready')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f731499f",
   "metadata": {},
   "source": [
    "## 7) Model architecture (Keras)\n",
    "\n",
    "We'll build a compact model suitable for TFLite: a small Conv1D + Dense or LSTM. Conv1D is typically smaller and faster on-device. The model inputs shape: (window_size, n_features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fadfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "    from tensorflow.keras import layers, models\n",
    "    inp = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(inp)\n",
    "    x = layers.Conv1D(filters=32, kernel_size=3, activation='relu', padding='same')(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    out = layers.Dense(1, activation='linear')(x)\n",
    "    model = models.Model(inputs=inp, outputs=out)\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "input_shape = (window_size, len(feature_cols))\n",
    "model = build_model(input_shape)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# Small utility to compute RMSE and R2\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((y_true - y_pred)**2))\n",
    "\n",
    "def r2_score_np(y_true, y_pred):\n",
    "    ss_res = np.sum((y_true - y_pred)**2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true))**2)\n",
    "    return 1 - ss_res/ss_tot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024e0929",
   "metadata": {},
   "source": [
    "## 8) Train model with callbacks and visualize learning\n",
    "\n",
    "We'll use EarlyStopping and ModelCheckpoint to save the best model. Training is intentionally small (few epochs) to keep Colab runs quick; increase epochs for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5601331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "os.makedirs('export', exist_ok=True)\n",
    "ckpt_path = 'export/best_model.h5'\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True),\n",
    "    ModelCheckpoint(ckpt_path, monitor='val_loss', save_best_only=True)\n",
    "]\n",
    "\n",
    "# Train\n",
    "epochs = 50\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.title('Training curves')\n",
    "plt.show()\n",
    "\n",
    "# Load best model saved by checkpoint\n",
    "model.load_weights(ckpt_path)\n",
    "\n",
    "# Evaluate on validation\n",
    "val_preds = model.predict(X_val).flatten()\n",
    "val_mse = np.mean((val_preds - y_val)**2)\n",
    "val_rmse = np.sqrt(val_mse)\n",
    "val_mae = np.mean(np.abs(val_preds - y_val))\n",
    "val_r2 = r2_score_np(y_val, val_preds)\n",
    "print(f'Validation RMSE: {val_rmse:.4f}, MAE: {val_mae:.4f}, R2: {val_r2:.4f}')\n",
    "\n",
    "# Plot predicted vs actual (last 200 points)\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.plot(y_val[-200:], label='actual')\n",
    "plt.plot(val_preds[-200:], label='predicted')\n",
    "plt.legend()\n",
    "plt.title('Actual vs Predicted (validation tail)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b61c167",
   "metadata": {},
   "source": [
    "## 9) Export SavedModel and HDF5\n",
    "\n",
    "We'll save the Keras model in SavedModel format and HDF5 (for convenience). We'll also save the feature order and scaler params to `export/` so the Flutter app knows the input order/normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0888c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export SavedModel\n",
    "saved_model_dir = 'export/saved_model'\n",
    "model.save(saved_model_dir, include_optimizer=False)\n",
    "print('SavedModel saved to', saved_model_dir)\n",
    "\n",
    "# Save HDF5\n",
    "model.save('export/model.h5')\n",
    "print('HDF5 saved to export/model.h5')\n",
    "\n",
    "# Save feature order\n",
    "import json\n",
    "with open('export/feature_order.json', 'w') as f:\n",
    "    json.dump(feature_cols, f)\n",
    "print('Saved feature_order.json')\n",
    "\n",
    "# Ensure scaler_params.npz exists (saved earlier)\n",
    "print('Export folder contents:')\n",
    "!ls -la export\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5118cfd1",
   "metadata": {},
   "source": [
    "## 10) Convert SavedModel to TFLite (float & quantized)\n",
    "\n",
    "We'll convert the saved model to TFLite float model and then use post-training quantization. For full integer quantization, we provide a representative dataset generator built from training sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fa908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to TFLite (float)\n",
    "import tensorflow as tf\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "tflite_model = converter.convert()\n",
    "open('export/model.tflite', 'wb').write(tflite_model)\n",
    "print('Float TFLite saved to export/model.tflite')\n",
    "\n",
    "# Representative dataset for quantization (use a subset of training data)\n",
    "def representative_dataset_gen():\n",
    "    for i in range(min(1000, X_train.shape[0])):\n",
    "        sample = X_train[i:i+1].astype(np.float32)\n",
    "        yield [sample]\n",
    "\n",
    "# Post-training quantization (dynamic range)\n",
    "converter_quant = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "converter_quant.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter_quant.representative_dataset = representative_dataset_gen\n",
    "converter_quant.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# Set input and output types to int8 for full integer quantization\n",
    "converter_quant.inference_input_type = tf.int8\n",
    "converter_quant.inference_output_type = tf.int8\n",
    "try:\n",
    "    tflite_quant = converter_quant.convert()\n",
    "    open('export/model_quant.tflite', 'wb').write(tflite_quant)\n",
    "    print('Quantized TFLite saved to export/model_quant.tflite')\n",
    "except Exception as e:\n",
    "    print('Quantization failed (fall back to dynamic range quantization):', e)\n",
    "    # Try dynamic range quantization\n",
    "    converter_dr = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "    converter_dr.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    tflite_dr = converter_dr.convert()\n",
    "    open('export/model_quant_dynamic.tflite', 'wb').write(tflite_dr)\n",
    "    print('Dynamic-range quantized TFLite saved to export/model_quant_dynamic.tflite')\n",
    "\n",
    "print('\\nExported files:')\n",
    "!ls -la export\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2388eef",
   "metadata": {},
   "source": [
    "## 11) Validate TFLite model locally in Python\n",
    "\n",
    "We'll run a few inference checks with both the Keras model and the TFLite model on the same validation samples and compare outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea5ce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to run tflite model\n",
    "import tensorflow as tf\n",
    "\n",
    "def run_tflite_model(tflite_path, X_input):\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    # Accept either int8 or float32 input types\n",
    "    inp_dtype = input_details[0]['dtype']\n",
    "    X = X_input.astype(np.float32)\n",
    "    if inp_dtype == np.int8:\n",
    "        # map float into int8 range using 127*X (not ideal, representative dataset needed for proper scaling)\n",
    "        X = (X * 1.0).astype(np.float32)\n",
    "    interpreter.set_tensor(input_details[0]['index'], X)\n",
    "    interpreter.invoke()\n",
    "    out = interpreter.get_tensor(output_details[0]['index'])\n",
    "    return out\n",
    "\n",
    "# Pick small slice from validation\n",
    "sample_X = X_val[:20].astype(np.float32)\n",
    "keras_out = model.predict(sample_X).flatten()\n",
    "\n",
    "# Float TFLite\n",
    "tflite_float_path = 'export/model.tflite'\n",
    "if os.path.exists(tflite_float_path):\n",
    "    tflite_out = run_tflite_model(tflite_float_path, sample_X)\n",
    "    print('Keras vs TFLite float first 5 preds:')\n",
    "    for k, t in zip(keras_out[:5], tflite_out.flatten()[:5]):\n",
    "        print(f'{k:.4f}  |  {t:.4f}')\n",
    "else:\n",
    "    print('Float TFLite not found')\n",
    "\n",
    "# Quantized\n",
    "tflite_q_path = 'export/model_quant.tflite'\n",
    "if not os.path.exists(tflite_q_path):\n",
    "    tflite_q_path = 'export/model_quant_dynamic.tflite' if os.path.exists('export/model_quant_dynamic.tflite') else None\n",
    "\n",
    "if tflite_q_path and os.path.exists(tflite_q_path):\n",
    "    try:\n",
    "        tflite_q_out = run_tflite_model(tflite_q_path, sample_X)\n",
    "        print('\\nKeras vs TFLite quant first 5 preds:')\n",
    "        for k, t in zip(keras_out[:5], tflite_q_out.flatten()[:5]):\n",
    "            print(f'{k:.4f}  |  {t:.4f}')\n",
    "    except Exception as e:\n",
    "        print('Quant TFLite run failed:', e)\n",
    "else:\n",
    "    print('Quantized TFLite not found')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1125f2e",
   "metadata": {},
   "source": [
    "## 12) Package artifacts for Flutter\n",
    "\n",
    "We'll create a zip containing:\n",
    "- model.tflite (or quantized),\n",
    "- feature_order.json,\n",
    "- scaler_params.npz,\n",
    "- a short `metadata.json` describing input shape and feature order.\n",
    "\n",
    "Download the zip or copy it to Drive for integration into your Flutter app under `models/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a5c91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, zipfile\n",
    "\n",
    "metadata = {\n",
    "    'input_shape': [window_size, len(feature_cols)],\n",
    "    'feature_order': feature_cols,\n",
    "    'scaler': 'StandardScaler',\n",
    "    'dtype': 'float32'\n",
    "}\n",
    "with open('export/metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f)\n",
    "\n",
    "zip_path = 'export/artifacts.zip'\n",
    "with zipfile.ZipFile(zip_path, 'w') as z:\n",
    "    for fname in ['feature_order.json', 'scaler_params.npz', 'metadata.json']:\n",
    "        if os.path.exists(f'export/{fname}'):\n",
    "            z.write(f'export/{fname}', arcname=fname)\n",
    "    # prefer quantized if available\n",
    "    if os.path.exists('export/model_quant.tflite'):\n",
    "        z.write('export/model_quant.tflite', arcname='model.tflite')\n",
    "    elif os.path.exists('export/model_quant_dynamic.tflite'):\n",
    "        z.write('export/model_quant_dynamic.tflite', arcname='model.tflite')\n",
    "    elif os.path.exists('export/model.tflite'):\n",
    "        z.write('export/model.tflite', arcname='model.tflite')\n",
    "\n",
    "print('Packaged artifacts to', zip_path)\n",
    "\n",
    "# Optionally copy to Drive\n",
    "drive_export_dir = '/content/drive/MyDrive/MamaAbbysMNVGMNT/export'\n",
    "try:\n",
    "    os.makedirs(drive_export_dir, exist_ok=True)\n",
    "    import shutil\n",
    "    shutil.copy(zip_path, os.path.join(drive_export_dir, 'artifacts.zip'))\n",
    "    print('Copied artifacts.zip to Drive:', drive_export_dir)\n",
    "except Exception as e:\n",
    "    print('Could not copy to Drive:', e)\n",
    "\n",
    "# Provide download link in Colab\n",
    "from google.colab import files\n",
    "print('Starting download...')\n",
    "files.download(zip_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8ded9a",
   "metadata": {},
   "source": [
    "## 13) Dart integration snippet (for `sales_page.dart` Forecast tab)\n",
    "\n",
    "Paste this into your Flutter project (e.g., a `ForecastService`) that loads `assets/model.tflite` and runs inference using `tflite_flutter` and `tflite_flutter_helper`.\n",
    "\n",
    "Note: you'll need to include the TFLite file in `pubspec.yaml` under `assets:` and add `tflite_flutter` and `tflite_flutter_helper` packages to `pubspec.yaml`.\n",
    "\n",
    "Example (pseudo-code):\n",
    "\n",
    "```dart\n",
    "// In ForecastService\n",
    "import 'dart:typed_data';\n",
    "import 'package:tflite_flutter/tflite_flutter.dart';\n",
    "\n",
    "class ForecastService {\n",
    "  late Interpreter _interpreter;\n",
    "  List<String> featureOrder; // load from feature_order.json\n",
    "  List<double> scalerMean, scalerScale; // load from npz converted to json\n",
    "\n",
    "  Future<void> loadModel() async {\n",
    "    _interpreter = await Interpreter.fromAsset('model.tflite');\n",
    "  }\n",
    "\n",
    "  Future<double> predictNext(List<double> featuresWindow) async {\n",
    "    // featuresWindow: flattened window_size * feature_count in row-major\n",
    "    final input = Float32List.fromList(featuresWindow); // shape [1, window, features]\n",
    "    final output = Float32List(1);\n",
    "    _interpreter.run(input, output);\n",
    "    return output[0];\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "In `sales_page.dart`, call your ForecastService's `forecastNext30Days()` (similar to existing import) and return a list of `DailyForecast(date, predictedSales)` for display in the Forecast tab.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66950c0",
   "metadata": {},
   "source": [
    "## 14) Optional: Serve model via a small REST API (Flask/FastAPI)\n",
    "\n",
    "If you prefer remote inference instead of embedding the model, you can run a small Flask or FastAPI server in Colab (not recommended for production). This section provides a sample FastAPI snippet (commented) and notes about deploying to a proper server or Cloud Run.\n",
    "\n",
    "(Implementation left as an exercise — uncomment to use in short-lived demos.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f7d329",
   "metadata": {},
   "source": [
    "## 15) Reproducibility & environment capture\n",
    "\n",
    "Save a `requirements.txt` snapshot for reproducibility (note Colab uses many preinstalled packages). Also include seed reminders and runtime notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4586cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pip freeze\n",
    "!pip freeze > export/requirements.txt\n",
    "print('Saved pip freeze to export/requirements.txt')\n",
    "\n",
    "# Final note to user\n",
    "print('\\nNotebook ready. Run cells in order. After export, copy model artifacts into your Flutter project `models/` and update pubspec.yaml with the model asset.')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
